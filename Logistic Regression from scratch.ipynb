{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression from scratch without using scikit learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "data = sm.datasets.fair.load_pandas().data\n",
    "data['affairs'] = (data.affairs > 0).astype(int)\n",
    "x = data.drop('affairs', axis = 1)\n",
    "y = data['affairs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Defining logistic regression class, with all functions ** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "class logisticRegression():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.coefficient = 0\n",
    "        \n",
    "    def sigmoid(self, array):\n",
    "        return 1 / (1 + np.exp(-array)).round(3)\n",
    "        \n",
    "    def fit(self, x, y, alpha, threshold):\n",
    "        \n",
    "        \"\"\" x: input pandas dataframe\n",
    "        y: output pandas series (Class labels)\n",
    " \n",
    "        step1: Check if the number of rows in x and y are the same. If not raise a value error with a message.\"\"\"\n",
    "        \n",
    "        if x.shape[0] != y.shape[0]:\n",
    "            raise ValueError(\"The number of rows in input dataframe and output series is not equal\")\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        \"\"\"step2: Check if there is any missing value in the dataset (both for x and y). \n",
    "               If there is, raise a value error with a message.\"\"\"\n",
    "        \n",
    "        if x.isnull().values.any() == True:\n",
    "            raise ValueError(\"The input dataframe should not have missing values\")\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        if y.isnull().values.any() == True:\n",
    "            raise ValueError(\"The output should not have missing values\")\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        \"\"\"step3: Check if there is any categorical value in x or y. If there is, raise a value error with a message.\"\"\"\n",
    "        for c in x.columns:\n",
    "            if x[c].dtype == 'object':\n",
    "                raise ValueError(\"Input dataframe should not have categorical variables\")\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        if y.dtype == 'object':\n",
    "                raise ValueError(\"The output should not be categorical\")\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        \"\"\"step4: Transform both x and y into numpy.arrays (it is easier to work with arrays for matrix operations).\"\"\"\n",
    "        x = x.values\n",
    "        y = y.values\n",
    "        y = y.reshape(6366,1)\n",
    "        \n",
    "        \"\"\"step5: Add bias to the input vector x. bias means add a column which is 1 across al the rows.\n",
    "               This will increase the number of columns of x by 1. x.shape[1] will increase by 1.\"\"\"\n",
    "        bias = np.ones((x.shape[0],1))\n",
    "        x = np.concatenate((bias, x), axis = 1)\n",
    "        \"\"\"step6: initialize self.coef.\n",
    "               You can initialize the coef randomly. \n",
    "               Use numpy.random.rand(size) or np.random.uniform(low=-1, high=1, size=(x.shape[1])).\n",
    "               Think about the size of the coefficent array. \n",
    "               Logically, you need to have a coefficient for each input variable as well as the bias.\"\"\" \n",
    "        \n",
    "        coefficient = np.random.uniform(low=-1, high=1, size=(x.shape[1],1))\n",
    "        \n",
    "        \"\"\"step7: create a list to save the cost values for each iteration.\"\"\"\n",
    "        cost = []\n",
    "        \n",
    "        \"\"\"step8: while not converged and iteration number > 10000\n",
    "                    calculate the predicted values\n",
    "                    calculate the error \n",
    "                    calculate the cost function and append it to the cost list\n",
    "                    calculate the gradient in a way that gradient is \n",
    "                                      gradient = (t(x) * (error))/(size_of_x) (number of rows)\n",
    "                    adjust the coef in a way that\n",
    "                                        coef = coef - alpha*gradient\n",
    "                    adjust alpha in a way that\n",
    "                                        alpha = alpha*0.95\"\"\"\n",
    "        iteration = 1\n",
    "        not_converged = True\n",
    "        \n",
    "        while not_converged:\n",
    "            product = np.dot(x,coefficient)\n",
    "            y_pred = self.sigmoid(product)\n",
    "            error = y - y_pred\n",
    "            a = np.sum((-y*np.log(y_pred+0.000001)) - ((1-y)*np.log(1-y_pred+0.00001)))/ x.shape[0]\n",
    "            cost = np.append(cost,a)\n",
    "            gradient = np.dot(x.T,error)/x.shape[0]\n",
    "            coefficient = coefficient - (alpha*gradient)\n",
    "            alpha = alpha * 0.95\n",
    "            iteration = iteration + 1\n",
    "            \n",
    "            \"\"\"step 8: Check if the convergence criteria is satisfied:\n",
    "                if you iterate at least as many times 10000\n",
    "                if the difference between the average of the last 5 cost values and the last cost value \n",
    "                is less than the threshold.\n",
    "                You will not need to return anything because you are working on the coefs, which are class attributes\n",
    "            \"\"\"\n",
    "            iteration = iteration + 1\n",
    "            b = abs(cost[-5:].mean() - cost[-1])\n",
    "            if b < threshold and iteration >5:\n",
    "                not_converged = False\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "        self.coefficient = coefficient\n",
    "    \n",
    "    \n",
    "    def predict_prob(self, x):\n",
    "        \n",
    "        \"\"\"Convert x into numpy aray and add bias\n",
    "        Check if size of self.coef is the same with the number of columns in x\n",
    "        Using x and self.coef, make the predictions\n",
    "        \"\"\"\n",
    "        \n",
    "        x = x.values\n",
    "        bias = np.ones((x.shape[0],1))\n",
    "        x = np.concatenate((bias, x), axis = 1)\n",
    "        product = np.dot(x,self.coefficient)\n",
    "        y_pred = (1/(1+np.exp(-product))).round(3)      \n",
    "        \n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "    \n",
    "    def predict_class(self, x):\n",
    "        \n",
    "        \"\"\"Make discrete predictions. Instead of returning probabilities return 0 or 1.\n",
    "        \"\"\"\n",
    "        \n",
    "        y_pred = self.predict_prob(x) \n",
    "        y_pred[y_pred>=0.5] = 1\n",
    "        y_pred[y_pred<0.5] = 0\n",
    "        \n",
    "        \n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "    def get_accuracy(self, x, y):\n",
    "        \n",
    "        \"\"\"Calculate the accuracy rate\n",
    "        number of true classification/total number of instances\n",
    "        number of true classification is True positive + True negative\n",
    "        \"\"\"\n",
    "        \n",
    "        y = y.values\n",
    "        y = y.reshape(6366,1)\n",
    "        y_pred = self.predict_class(x)\n",
    "        accuracy = float(np.sum(y==y_pred)/float(x.shape[0])) * 100\n",
    "        \n",
    "        \n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Testing the Accuracy **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.75054979579014\n",
      "[[ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " ..., \n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prateikm\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: RuntimeWarning: overflow encountered in exp\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\Prateikm\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: RuntimeWarning: overflow encountered in multiply\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\Prateikm\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:119: RuntimeWarning: overflow encountered in exp\n"
     ]
    }
   ],
   "source": [
    "data['affairs'] = (data.affairs > 0).astype(int)\n",
    "x = data.drop('affairs', axis = 1)\n",
    "y = data['affairs']\n",
    "LR = logisticRegression()\n",
    "LR.fit(x, y, 0.99, 0.001) \n",
    "print(LR.get_accuracy(x, y))\n",
    "class_labels_pred = LR.predict_class(x)\n",
    "print(class_labels_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
